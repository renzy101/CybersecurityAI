{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e84g1YoseoE"
      },
      "source": [
        "# TextAttack CLARE\n",
        "\n",
        "Replication of the training, evaluating, and attacking a model using TextAttack recipe CLARE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AQTkpf9RslEA",
        "outputId": "1e223b3f-afc8-4460-b325-1ebcfbe35efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textattack[optional,tensorflow]\n",
            "  Downloading textattack-0.3.9-py3-none-any.whl (436 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bert-score>=0.3.5 (from textattack[optional,tensorflow])\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (0.6.2)\n",
            "Collecting flair (from textattack[optional,tensorflow])\n",
            "  Downloading flair-0.13.0-py3-none-any.whl (387 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.2/387.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (3.13.1)\n",
            "Collecting language-tool-python (from textattack[optional,tensorflow])\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Collecting lemminflect (from textattack[optional,tensorflow])\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lru-dict (from textattack[optional,tensorflow])\n",
            "  Downloading lru_dict-1.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting datasets>=2.4.0 (from textattack[optional,tensorflow])\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (1.11.4)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (4.35.2)\n",
            "Collecting terminaltables (from textattack[optional,tensorflow])\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (4.66.1)\n",
            "Collecting word2number (from textattack[optional,tensorflow])\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting num2words (from textattack[optional,tensorflow])\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (10.1.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (1.7.1)\n",
            "Collecting pinyin>=0.4.0 (from textattack[optional,tensorflow])\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (0.42.1)\n",
            "Collecting OpenHowNet (from textattack[optional,tensorflow])\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Collecting pycld2 (from textattack[optional,tensorflow])\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting click<8.1.0 (from textattack[optional,tensorflow])\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.9.1 (from textattack[optional,tensorflow])\n",
            "  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from textattack[optional,tensorflow]) (0.15.0)\n",
            "Collecting tensorflow-text>=2 (from textattack[optional,tensorflow])\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX (from textattack[optional,tensorflow])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator==2.9.0 (from textattack[optional,tensorflow])\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers==2.2.0 (from textattack[optional,tensorflow])\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stanza (from textattack[optional,tensorflow])\n",
            "  Downloading stanza-1.7.0-py3-none-any.whl (933 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m933.2/933.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting visdom (from textattack[optional,tensorflow])\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb (from textattack[optional,tensorflow])\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim==4.1.2 (from textattack[optional,tensorflow])\n",
            "  Downloading gensim-4.1.2.tar.gz (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.1.2->textattack[optional,tensorflow]) (6.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.0->textattack[optional,tensorflow]) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.0->textattack[optional,tensorflow]) (1.2.2)\n",
            "Collecting sentencepiece (from sentence-transformers==2.2.0->textattack[optional,tensorflow])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.0->textattack[optional,tensorflow]) (0.19.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.1->textattack[optional,tensorflow])\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.1->textattack[optional,tensorflow])\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (1.59.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (3.9.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.1->textattack[optional,tensorflow])\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.1->textattack[optional,tensorflow])\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (23.2)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.9.1->textattack[optional,tensorflow])\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.1->textattack[optional,tensorflow])\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (0.34.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.1->textattack[optional,tensorflow]) (1.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack[optional,tensorflow]) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack[optional,tensorflow]) (3.7.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[optional,tensorflow]) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.4.0->textattack[optional,tensorflow])\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.4.0->textattack[optional,tensorflow])\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[optional,tensorflow]) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.4.0->textattack[optional,tensorflow])\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[optional,tensorflow]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[optional,tensorflow]) (3.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack[optional,tensorflow]) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack[optional,tensorflow]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack[optional,tensorflow]) (2023.3.post1)\n",
            "INFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-text>=2 (from textattack[optional,tensorflow])\n",
            "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_text-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_text-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_text-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_text-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack[optional,tensorflow]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack[optional,tensorflow]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack[optional,tensorflow]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack[optional,tensorflow]) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack[optional,tensorflow]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack[optional,tensorflow]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack[optional,tensorflow]) (0.4.1)\n",
            "Collecting boto3>=1.20.27 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading boto3-1.33.13-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting conllu>=4.0 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[optional,tensorflow]) (4.6.6)\n",
            "INFO: pip is looking at multiple versions of flair to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting flair (from textattack[optional,tensorflow])\n",
            "  Downloading flair-0.12.2-py3-none-any.whl (373 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.1/373.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting segtok>=1.5.7 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting mpld3==0.3 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.5/788.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sqlitedict>=1.6.0 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: hyperopt>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from flair->textattack[optional,tensorflow]) (0.2.7)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from flair->textattack[optional,tensorflow]) (0.9.0)\n",
            "Collecting langdetect (from flair->textattack[optional,tensorflow])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from flair->textattack[optional,tensorflow]) (4.9.3)\n",
            "Collecting janome (from flair->textattack[optional,tensorflow])\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gdown==4.4.0 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wikipedia-api (from flair->textattack[optional,tensorflow])\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting pptree (from flair->textattack[optional,tensorflow])\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-revgrad (from flair->textattack[optional,tensorflow])\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.1 (from flair->textattack[optional,tensorflow])\n",
            "  Downloading transformer_smaller_training_vocab-0.3.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==4.4.0->flair->textattack[optional,tensorflow]) (4.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack[optional,tensorflow]) (1.3.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->textattack[optional,tensorflow])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting anytree (from OpenHowNet->textattack[optional,tensorflow])\n",
            "  Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji (from stanza->textattack[optional,tensorflow])\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza->textattack[optional,tensorflow]) (0.10.2)\n",
            "INFO: pip is looking at multiple versions of tensorboardx to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorboardX (from textattack[optional,tensorflow])\n",
            "  Downloading tensorboardX-2.6.1-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom->textattack[optional,tensorflow]) (6.3.2)\n",
            "Collecting jsonpatch (from visdom->textattack[optional,tensorflow])\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom->textattack[optional,tensorflow]) (1.7.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom->textattack[optional,tensorflow]) (9.4.0)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->textattack[optional,tensorflow])\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->textattack[optional,tensorflow]) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->textattack[optional,tensorflow])\n",
            "  Downloading sentry_sdk-1.39.0-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.0/254.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->textattack[optional,tensorflow])\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb->textattack[optional,tensorflow])\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->textattack[optional,tensorflow]) (1.4.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1->textattack[optional,tensorflow]) (0.42.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[optional,tensorflow]) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[optional,tensorflow]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[optional,tensorflow]) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[optional,tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[optional,tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack[optional,tensorflow]) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->textattack[optional,tensorflow])\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack[optional,tensorflow]) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack[optional,tensorflow]) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.7->flair->textattack[optional,tensorflow]) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[optional,tensorflow]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[optional,tensorflow]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[optional,tensorflow]) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[optional,tensorflow]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack[optional,tensorflow]) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack[optional,tensorflow]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack[optional,tensorflow]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack[optional,tensorflow]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack[optional,tensorflow]) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.0->textattack[optional,tensorflow]) (3.2.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow]) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow])\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow]) (3.5.1)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow])\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow])\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow]) (3.0.1)\n",
            "Collecting botocore<1.34.0,>=1.33.13 (from boto3>=1.20.27->flair->textattack[optional,tensorflow])\n",
            "  Downloading botocore-1.33.13-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair->textattack[optional,tensorflow])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3>=1.20.27->flair->textattack[optional,tensorflow])\n",
            "  Downloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack[optional,tensorflow]) (0.2.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack[optional,tensorflow]) (2.1.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom->textattack[optional,tensorflow])\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack[optional,tensorflow]) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->textattack[optional,tensorflow])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow]) (1.3.1)\n",
            "Collecting accelerate>=0.20.3 (from transformers>=4.30.0->textattack[optional,tensorflow])\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==4.4.0->flair->textattack[optional,tensorflow]) (2.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1->textattack[optional,tensorflow]) (3.2.2)\n",
            "Building wheels for collected packages: gensim, sentence-transformers, pinyin, gdown, mpld3, pycld2, visdom, word2number, docopt, sqlitedict, langdetect, pptree\n",
            "  Building wheel for gensim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gensim: filename=gensim-4.1.2-cp310-cp310-linux_x86_64.whl size=25996773 sha256=5ac25c85b7978da97bb4a2654419c42f62bc8848439f7b5250005a0d0f8fd9e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/35/4e/dca2954de21981d0a137ff930239f0767403a617e32f19f04f\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120730 sha256=c8511f8f07375df9f5070e2b34761bc01c9712a9e9f81f455d7ad2b5638fe7b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/0d/ea/b89527acdff464eb4476a68607f6ca4e8ad24b6f6d5e8cfacb\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630476 sha256=f089f6b4885396f4e7af2dc1127f939350718338228c8bca289428efa8660026\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14758 sha256=c188fc1be076a07d3787795b1fe0c2b041a0ccd20fa1977dd57d8d8678a6e994\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/0b/3f/6ddf67a417a5b400b213b0bb772a50276c199a386b12c06bfc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116686 sha256=5df04ee2f2941226a6883db48933911fe791b5ce828a3c52506f61504c436631\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/92/f7/45d9aac5dcfb1c2a1761a272365599cc7ba1050ce211a3fd9a\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp310-cp310-linux_x86_64.whl size=9904039 sha256=39addea99f2b34159cc1cf832087da68e4ded8692193270ac5c657e23198092a\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/81/31/240c89c845e008a93d98542325270007de595bfd356eb0b06c\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408194 sha256=53f23aadbf30174a5fa71b4322ca50ac9ee9ba108c77b99c61260eec04ca6a2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=84f8b6dcc93b28ff1c5258a644a374f5bc4276aaed6b7dbcd74a4bddb9432c00\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=56d5097bc0d0c26f2587c3ad85333aeeaf9a01f02b14845397dbdcb7852c0f55\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=d8146d9ad2d7258c8261faac2e46f964bae10aefa94e81951e97273bbbf8e12e\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=baab8a2532372c01aaf6a21c8458953999813806091c765aa24d48b78b2f46c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=154992f5e1b0bafb3a6ab39325aab0e8a559ba892a58b3dd002532a7fd6da51b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "Successfully built gensim sentence-transformers pinyin gdown mpld3 pycld2 visdom word2number docopt sqlitedict langdetect pptree\n",
            "Installing collected packages: word2number, tensorboard-plugin-wit, sqlitedict, sentencepiece, pycld2, pptree, pinyin, mpld3, keras, janome, flatbuffers, docopt, terminaltables, tensorflow-estimator, tensorboard-data-server, smmap, setproctitle, sentry-sdk, segtok, pyarrow-hotfix, protobuf, num2words, lru-dict, lemminflect, langdetect, keras-preprocessing, jsonpointer, jmespath, gast, ftfy, emoji, docker-pycreds, dill, deprecated, conllu, click, anytree, wikipedia-api, tensorboardX, OpenHowNet, multiprocess, language-tool-python, jsonpatch, gitdb, gensim, botocore, visdom, stanza, s3transfer, pytorch-revgrad, google-auth-oauthlib, GitPython, gdown, bpemb, accelerate, wandb, tensorboard, datasets, boto3, tensorflow, sentence-transformers, bert-score, transformer-smaller-training-vocab, tensorflow-text, flair, textattack\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.5.26\n",
            "    Uninstalling flatbuffers-23.5.26:\n",
            "      Successfully uninstalled flatbuffers-23.5.26\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.2\n",
            "    Uninstalling gensim-4.3.2:\n",
            "      Successfully uninstalled gensim-4.3.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.14.0\n",
            "    Uninstalling tensorflow-2.14.0:\n",
            "      Successfully uninstalled tensorflow-2.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 OpenHowNet-2.0 accelerate-0.25.0 anytree-2.12.1 bert-score-0.3.13 boto3-1.33.13 botocore-1.33.13 bpemb-0.3.4 click-8.0.4 conllu-4.5.3 datasets-2.15.0 deprecated-1.2.14 dill-0.3.7 docker-pycreds-0.4.0 docopt-0.6.2 emoji-2.9.0 flair-0.12.2 flatbuffers-1.12 ftfy-6.1.3 gast-0.4.0 gdown-4.4.0 gensim-4.1.2 gitdb-4.0.11 google-auth-oauthlib-0.4.6 janome-0.5.0 jmespath-1.0.1 jsonpatch-1.33 jsonpointer-2.4 keras-2.9.0 keras-preprocessing-1.1.2 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.3.0 mpld3-0.3 multiprocess-0.70.15 num2words-0.5.13 pinyin-0.4.0 pptree-3.1 protobuf-3.19.6 pyarrow-hotfix-0.6 pycld2-0.41 pytorch-revgrad-0.2.0 s3transfer-0.8.2 segtok-1.5.11 sentence-transformers-2.2.0 sentencepiece-0.1.99 sentry-sdk-1.39.0 setproctitle-1.3.3 smmap-5.0.1 sqlitedict-2.1.0 stanza-1.7.0 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.6 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-text-2.9.0 terminaltables-3.1.10 textattack-0.3.9 transformer-smaller-training-vocab-0.3.3 visdom-0.2.4 wandb-0.16.1 wikipedia-api-0.6.0 word2number-1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install textattack[tensorflow,optional]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRU0ibw3etFs",
        "outputId": "c739a785-5c50-4e05-ad9f-e5adfec86546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.14\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.6.3)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow==2.14)\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (23.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.14)\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14) (1.59.3)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow==2.14)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.17.3)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow==2.14)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14) (3.2.2)\n",
            "Installing collected packages: flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.1\n",
            "    Uninstalling tensorflow-2.9.1:\n",
            "      Successfully uninstalled tensorflow-2.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-text 2.9.0 requires tensorflow<2.10,>=2.9.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-23.5.26 google-auth-oauthlib-1.0.0 keras-2.14.0 protobuf-4.25.1 tensorboard-2.14.1 tensorboard-data-server-0.7.2 tensorflow-2.14.0 tensorflow-estimator-2.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONayD5EJseoG"
      },
      "source": [
        "## Training\n",
        "\n",
        "First, we're going to train a model. TextAttack integrates directly with [transformers](https://github.com/huggingface/transformers/) and [datasets](https://github.com/huggingface/datasets) to train any of the `transformers` pre-trained models on datasets from `datasets`.\n",
        "\n",
        "Let's use the AG News dataset: it's relatively short, and showcases the key features of `textattack train`. Let's take a look at the dataset using `textattack peek-dataset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7284d6d-2b1d-4f75-8efe-f1a16431ecb9",
        "id": "Lrh-tSHKvSRl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34;1mtextattack\u001b[0m: Updating TextAttack package dependencies.\n",
            "\u001b[34;1mtextattack\u001b[0m: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json: 370kB [00:00, 14.8MB/s]        \n",
            "2023-12-13 18:12:57 INFO: Downloading default packages for language: en (English) ...\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.7.0/models/default.zip: 100% 527M/527M [00:04<00:00, 114MB/s]\n",
            "2023-12-13 18:13:08 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1353, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\", line 75, in <module>\n",
            "    from .table_question_answering import TableQuestionAnsweringArgumentHandler, TableQuestionAnsweringPipeline\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/table_question_answering.py\", line 26, in <module>\n",
            "    import tensorflow_probability as tfp\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_probability/__init__.py\", line 20, in <module>\n",
            "    from tensorflow_probability import substrates\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_probability/substrates/__init__.py\", line 17, in <module>\n",
            "    from tensorflow_probability.python.internal import all_util\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py\", line 138, in <module>\n",
            "    dir(globals()[pkg_name])  # Forces loading the package from its lazy loader.\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\", line 57, in __dir__\n",
            "    module = self._load()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\", line 37, in _load\n",
            "    self._on_first_access()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py\", line 59, in _validate_tf_environment\n",
            "    raise ImportError(\n",
            "ImportError: This version of TensorFlow Probability requires TensorFlow version >= 2.14; Detected an installation of version 2.9.1. Please upgrade TensorFlow to proceed.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/textattack\", line 5, in <module>\n",
            "    from textattack.commands.textattack_cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/__init__.py\", line 11, in <module>\n",
            "    from .attack_args import AttackArgs, CommandLineAttackArgs\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/attack_args.py\", line 16, in <module>\n",
            "    from .attack import Attack\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/attack.py\", line 19, in <module>\n",
            "    from textattack.constraints import Constraint, PreTransformationConstraint\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/constraints/__init__.py\", line 24, in <module>\n",
            "    from . import grammaticality\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/constraints/grammaticality/__init__.py\", line 10, in <module>\n",
            "    from . import language_models\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/constraints/grammaticality/language_models/__init__.py\", line 10, in <module>\n",
            "    from .google_language_model import Google1BillionWordsLanguageModel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/constraints/grammaticality/language_models/google_language_model/__init__.py\", line 8, in <module>\n",
            "    from .google_language_model import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/constraints/grammaticality/language_models/google_language_model/google_language_model.py\", line 11, in <module>\n",
            "    from textattack.transformations import WordSwap\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/transformations/__init__.py\", line 11, in <module>\n",
            "    from .word_swaps import *\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/transformations/word_swaps/__init__.py\", line 11, in <module>\n",
            "    from .chn_transformations import *\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/transformations/word_swaps/chn_transformations/__init__.py\", line 10, in <module>\n",
            "    from .chinese_word_swap_masked import ChineseWordSwapMaskedLM\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/textattack/transformations/word_swaps/chn_transformations/chinese_word_swap_masked.py\", line 6, in <module>\n",
            "    from transformers import pipeline\n",
            "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1343, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1355, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n",
            "This version of TensorFlow Probability requires TensorFlow version >= 2.14; Detected an installation of version 2.9.1. Please upgrade TensorFlow to proceed.\n"
          ]
        }
      ],
      "source": [
        "!textattack peek-dataset --dataset-from-huggingface ag_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spS2eW5WseoG",
        "outputId": "e6c73ad8-3dc9-438f-f7cc-b3227c374b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34;1mtextattack\u001b[0m: Updating TextAttack package dependencies.\n",
            "\u001b[34;1mtextattack\u001b[0m: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json: 370kB [00:00, 14.7MB/s]        \n",
            "2023-12-12 16:35:10 INFO: Downloading default packages for language: en (English) ...\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.7.0/models/default.zip: 100% 527M/527M [00:01<00:00, 266MB/s]\n",
            "2023-12-12 16:35:19 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2023-12-12 16:35:21.222977: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-12 16:35:21.223032: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-12 16:35:21.223073: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-12 16:35:22.204321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 4.06k/4.06k [00:00<00:00, 18.7MB/s]\n",
            "Downloading metadata: 100% 2.65k/2.65k [00:00<00:00, 13.1MB/s]\n",
            "Downloading readme: 100% 7.95k/7.95k [00:00<00:00, 29.9MB/s]\n",
            "Downloading data: 29.5MB [00:00, 58.8MB/s]\n",
            "Downloading data: 1.86MB [00:00, 29.3MB/s]      \n",
            "Generating train split: 100% 120000/120000 [00:03<00:00, 33576.74 examples/s]\n",
            "Generating test split: 100% 7600/7600 [00:00<00:00, 37596.00 examples/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mag_news\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Number of samples: \u001b[94m120000\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: Number of words per input:\n",
            "\u001b[34;1mtextattack\u001b[0m: \ttotal:   \u001b[94m4653536\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tmean:    \u001b[94m38.78\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tstd:     \u001b[94m11.18\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tmin:     \u001b[94m11\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tmax:     \u001b[94m178\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: Dataset lowercased: \u001b[94mFalse\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: First sample:\n",
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again. \n",
            "\n",
            "\u001b[34;1mtextattack\u001b[0m: Last sample:\n",
            "Nets get Carter from Raptors INDIANAPOLIS -- All-Star Vince Carter was traded by the Toronto Raptors to the New Jersey Nets for Alonzo Mourning, Eric Williams, Aaron Williams, and a pair of first-round draft picks yesterday. \n",
            "\n",
            "\u001b[34;1mtextattack\u001b[0m: Found 4 distinct outputs.\n",
            "\u001b[34;1mtextattack\u001b[0m: Most common outputs:\n",
            "\t 2      (30000)\n",
            "\t 3      (30000)\n",
            "\t 1      (30000)\n",
            "\t 0      (30000)\n"
          ]
        }
      ],
      "source": [
        "!textattack peek-dataset --dataset-from-huggingface ag_news"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uguqpjnLseoI"
      },
      "source": [
        "The dataset looks good! It's lowercased already, so we'll make sure our model is uncased. The longest input is 178 words, so we can cap our maximum sequence length (`--model-max-length`) at 180.\n",
        "\n",
        "We'll train [`distilbert-base-uncased`](https://huggingface.co/transformers/model_doc/distilbert.html), since it's a relatively small model, and a good example of how we integrate with `transformers`.\n",
        "\n",
        "So we have our command:\n",
        "\n",
        "```bash\n",
        "textattack train                      \\ # Train a model with TextAttack\n",
        "    --model distilbert-base-uncased   \\ # Using distilbert, uncased version, from `transformers`\n",
        "    --dataset rotten_tomatoes         \\ # On the Rotten Tomatoes dataset\n",
        "    --model-num-labels 4              \\ # That has 4 labels\n",
        "    --model-max-length 180             \\ # With a maximum sequence length of 180\n",
        "    --per-device-train-batch-size 128 \\ # And batch size of 128\n",
        "    --num-epochs 3                    \\ # For 3 epochs\n",
        "```\n",
        "\n",
        "Now let's run it (please remember to use GPU if you have access):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnCqQmGoU4CK"
      },
      "outputs": [],
      "source": [
        "#ds = tfds.load('huggingface:yelp_review_full/yelp_review_full')\n",
        "#distilroberta-base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY33W9aWseoI",
        "outputId": "b315c0de-b082-4ff9-8ce5-b81934e27235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-12 17:32:34.511143: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-12 17:32:34.511208: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-12 17:32:34.511253: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-12 17:32:36.048696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading transformers AutoModelForSequenceClassification: distilbert-base-uncased\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mag_news\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mag_news\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Writing logs to ./outputs/2023-12-12-17-32-40-485239/train_log.txt.\n",
            "\u001b[34;1mtextattack\u001b[0m: Wrote original training args to ./outputs/2023-12-12-17-32-40-485239/training_args.json.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34;1mtextattack\u001b[0m: ***** Running training *****\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num examples = 120000\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num epochs = 3\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num clean epochs = 3\n",
            "\u001b[34;1mtextattack\u001b[0m:   Instantaneous batch size per device = 128\n",
            "\u001b[34;1mtextattack\u001b[0m:   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "\u001b[34;1mtextattack\u001b[0m:   Gradient accumulation steps = 1\n",
            "\u001b[34;1mtextattack\u001b[0m:   Total optimization steps = 2814\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 1\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 1/3\n",
            "Loss 0.32800: 100% 938/938 [25:37<00:00,  1.64s/it]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 89.01%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 93.68%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2023-12-12-17-32-40-485239/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 2\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 2/3\n",
            "Loss 0.23236: 100% 938/938 [25:37<00:00,  1.64s/it]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 95.37%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 94.50%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2023-12-12-17-32-40-485239/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 3\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 3/3\n",
            "Loss 0.18376: 100% 938/938 [25:38<00:00,  1.64s/it]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 97.10%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 94.58%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2023-12-12-17-32-40-485239/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: Wrote README to ./outputs/2023-12-12-17-32-40-485239/README.md.\n"
          ]
        }
      ],
      "source": [
        "!textattack train --model-name-or-path distilbert-base-uncased --dataset ag_news --model-num-labels 4 --model-max-length 180 --per-device-train-batch-size 128 --num-epochs 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xzv3BGLseoI"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We successfully fine-tuned `distilbert-base-cased` for 3 epochs. Now let's evaluate it using `textattack eval`. This is as simple as providing the path to the pretrained model (that you just obtain from running the above command!) to `--model`, along with the number of evaluation samples. `textattack eval` will automatically load the evaluation data from training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGYR_W6DseoJ",
        "outputId": "3bd8b0ed-c81e-4e89-a499-0e709657e503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-12 19:08:04.142284: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-12 19:08:04.142343: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-12 19:08:04.142384: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-12 19:08:05.258179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mag_news\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Got 1000 predictions.\n",
            "\u001b[34;1mtextattack\u001b[0m: Correct 942/1000 (\u001b[94m94.20%\u001b[0m)\n"
          ]
        }
      ],
      "source": [
        "!textattack eval --num-examples 1000 --model ./outputs/2023-12-12-17-32-40-485239/best_model/ --dataset-from-huggingface ag_news --dataset-split test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFPkCZShseoJ"
      },
      "source": [
        "Awesome -- we were able to train a model up to 94.20% accuracy on the test dataset – with only a single command!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWglEuvUseoK"
      },
      "source": [
        "## Attack\n",
        "\n",
        "Finally, let's attack our pre-trained model.  We can do this by passing `--recipe clare` to `textattack attack`.\n",
        "\n",
        "> *Warning*: We're printing out 100 examples and, if the attack succeeds, their perturbations. The output of this command is going to be quite long!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL-Bo1bgseoK",
        "outputId": "ac126600-5756-4b1d-a618-af5926a21c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-12 20:23:44.003529: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-12 20:23:44.003589: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-12 20:23:44.003628: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-12 20:23:45.513206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mag_news\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
            "Attack(\n",
            "  (search_method): GreedySearch\n",
            "  (goal_function):  UntargetedClassification\n",
            "  (transformation):  CompositeTransformation(\n",
            "    (0): WordSwapMaskedLM(\n",
            "        (method):  bae\n",
            "        (masked_lm_name):  RobertaForCausalLM\n",
            "        (max_length):  512\n",
            "        (max_candidates):  50\n",
            "        (min_confidence):  0.0005\n",
            "      )\n",
            "    (1): WordInsertionMaskedLM(\n",
            "        (masked_lm_name):  RobertaForCausalLM\n",
            "        (max_length):  512\n",
            "        (max_candidates):  50\n",
            "        (min_confidence):  0.0\n",
            "      )\n",
            "    (2): WordMergeMaskedLM(\n",
            "        (masked_lm_name):  RobertaForCausalLM\n",
            "        (max_length):  512\n",
            "        (max_candidates):  50\n",
            "        (min_confidence):  0.005\n",
            "      )\n",
            "    )\n",
            "  (constraints): \n",
            "    (0): UniversalSentenceEncoder(\n",
            "        (metric):  cosine\n",
            "        (threshold):  0.7\n",
            "        (window_size):  15\n",
            "        (skip_text_shorter_than_window):  True\n",
            "        (compare_against_original):  True\n",
            "      )\n",
            "    (1): RepeatModification\n",
            "    (2): StopwordModification\n",
            "  (is_black_box):  True\n",
            ") \n",
            "\n",
            "  0% 0/100 [00:00<?, ?it/s]2023-12-12 20:24:09,235 SequenceTagger predicts: Dictionary with 19 tags: <unk>, NOUN, VERB, PUNCT, ADP, DET, PROPN, PRON, ADJ, ADV, CCONJ, PART, NUM, AUX, INTJ, SYM, X, <START>, <STOP>\n",
            "2023-12-12 20:24:09.652834: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "  1% 1/100 [00:51<1:25:39, 51.92s/it]--------------------------------------------- Result 1 ---------------------------------------------\n",
            "\u001b[94mBusiness (99%)\u001b[0m --> \u001b[92mSports (81%)\u001b[0m\n",
            "\n",
            "Fears for T N pension after talks Unions representing \u001b[94mworkers\u001b[0m at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\n",
            "\n",
            "Fears for T N pension after talks Unions representing \u001b[92mreferees\u001b[0m at Turner   Newall say they are 'disappointed' after talks with stricken \u001b[92mFifa\u001b[0m parent firm Federal Mogul.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:   2% 2/100 [05:10<4:13:16, 155.06s/it]--------------------------------------------- Result 2 ---------------------------------------------\n",
            "\u001b[35mSci/tech (100%)\u001b[0m --> \u001b[91mWorld (70%)\u001b[0m\n",
            "\n",
            "The Race is On: Second Private Team Sets Launch Date for \u001b[35mHuman\u001b[0m Spaceflight (\u001b[35mSPACE\u001b[0m.\u001b[35mcom\u001b[0m) \u001b[35mSPACE\u001b[0m.com - TORONTO, \u001b[35mCanada\u001b[0m -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its \u001b[35mmanned\u001b[0m rocket.\n",
            "\n",
            "The \u001b[91mNuclear\u001b[0m Race is On: Second Private Team Sets Launch Date for \u001b[91mLong\u001b[0m Spaceflight (\u001b[91mwww\u001b[0m.\u001b[91mUN\u001b[0m) \u001b[91mCNBC\u001b[0m.com - TORONTO, \u001b[91mPakistan\u001b[0m -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, \u001b[91mIran\u001b[0m has officially announced the first\\launch date for its \u001b[91mnuclear\u001b[0m rocket.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:   3% 3/100 [06:05<3:17:12, 121.98s/it]--------------------------------------------- Result 3 ---------------------------------------------\n",
            "\u001b[35mSci/tech (100%)\u001b[0m --> \u001b[91mWorld (98%)\u001b[0m\n",
            "\n",
            "Ky. Company Wins Grant to Study Peptides (\u001b[35mAP\u001b[0m) \u001b[35mAP\u001b[0m - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\n",
            "\n",
            "Ky. Company Wins Grant to Study Peptides (\u001b[91mAFP\u001b[0m) \u001b[91mAFP\u001b[0m - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:   4% 4/100 [06:47<2:42:57, 101.85s/it]--------------------------------------------- Result 4 ---------------------------------------------\n",
            "\u001b[35mSci/tech (100%)\u001b[0m --> \u001b[92mSports (100%)\u001b[0m\n",
            "\n",
            "Prediction Unit Helps \u001b[35mForecast\u001b[0m \u001b[35mWildfires\u001b[0m (AP) AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.\n",
            "\n",
            "Prediction Unit Helps \u001b[92mNFL\u001b[0m (AP) AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 4 / 0 / 0 / 4:   5% 5/100 [07:37<2:24:56, 91.54s/it] --------------------------------------------- Result 5 ---------------------------------------------\n",
            "\u001b[35mSci/tech (99%)\u001b[0m --> \u001b[91mWorld (83%)\u001b[0m\n",
            "\n",
            "Calif. Aims to Limit Farm-Related Smog (\u001b[35mAP\u001b[0m) AP - Southern California's smog-fighting agency went after emissions of the bovine variety Friday, adopting the nation's first rules to reduce air pollution from dairy cow manure.\n",
            "\n",
            "Calif. Aims to Limit Farm-Related Smog (\u001b[91mAFP\u001b[0m) \u001b[91mAFP\u001b[0m AP - Southern California's smog-fighting agency went after emissions of the bovine variety Friday, adopting the nation's first rules to reduce air pollution from dairy cow manure.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 5 / 0 / 0 / 5:   6% 6/100 [17:40<4:36:50, 176.71s/it]--------------------------------------------- Result 6 ---------------------------------------------\n",
            "\u001b[35mSci/tech (98%)\u001b[0m --> \u001b[94mBusiness (51%)\u001b[0m\n",
            "\n",
            "Open Letter Against British \u001b[35mCopyright\u001b[0m Indoctrination in Schools The British \u001b[35mDepartment\u001b[0m for Education and Skills (DfES) \u001b[35mrecently\u001b[0m launched a \"Music Manifesto\" campaign, with the ostensible intention of educating the next generation of British musicians. Unfortunately, they also teamed up with the music industry (EMI, and various artists) to make this popular. EMI has apparently negotiated their end well, so that children in our schools will now be indoctrinated about the illegality of downloading music.The ignorance and audacity of this got to me a little, so I wrote an open letter to the \u001b[35mDfES\u001b[0m about it. Unfortunately, it's pedantic, as I suppose you have to be when writing to goverment representatives. But I hope you find it useful, and perhaps feel inspired to do something similar, if or when the same thing has happened in your area.\n",
            "\n",
            "Open Letter Against British \u001b[94mLabel\u001b[0m Indoctrination in Schools The British \u001b[94mFederation\u001b[0m for \u001b[94mFiscal\u001b[0m Education and Skills (DfES) \u001b[94myesterday\u001b[0m launched a \"Music Manifesto\" campaign, with the ostensible intention of educating the next generation of British musicians. Unfortunately, they also teamed up with the music industry (EMI, and various artists) to make this popular. EMI has apparently negotiated their end well, so that children in our schools will now be indoctrinated about the illegality of downloading music.The ignorance and audacity of this got to me a little, so I wrote an open letter to \u001b[94mAudit\u001b[0m the \u001b[94mTreasury\u001b[0m about it. Unfortunately, it's pedantic, as I suppose you have to be when writing to goverment representatives. But I hope you find it useful, and perhaps feel inspired to do something similar, if or when the same thing has happened in your area.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 6 / 0 / 0 / 6:   7% 7/100 [1:07:50<15:01:26, 581.57s/it]--------------------------------------------- Result 7 ---------------------------------------------\n",
            "\u001b[35mSci/tech (100%)\u001b[0m --> \u001b[91m[FAILED]\u001b[0m\n",
            "\n",
            "Loosing the War on Terrorism \\\\\"Sven Jaschan, self-confessed author of the Netsky and Sasser viruses, is\\responsible for 70 percent of virus infections in 2004, according to a six-month\\virus roundup published Wednesday by antivirus company Sophos.\"\\\\\"The 18-year-old Jaschan was taken into custody in Germany in May by police who\\said he had admitted programming both the Netsky and Sasser worms, something\\experts at Microsoft confirmed. (A Microsoft antivirus reward program led to the\\teenager's arrest.) During the five months preceding Jaschan's capture, there\\were at least 25 variants of Netsky and one of the port-scanning network worm\\Sasser.\"\\\\\"Graham Cluley, senior technology consultant at Sophos, said it was staggeri ...\\\\\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 6 / 1 / 0 / 7:   8% 8/100 [1:23:02<15:55:00, 622.84s/it]--------------------------------------------- Result 8 ---------------------------------------------\n",
            "\u001b[35mSci/tech (100%)\u001b[0m --> \u001b[94mBusiness (57%)\u001b[0m\n",
            "\n",
            "FOAFKey: FOAF, PGP, Key Distribution, and Bloom Filters \\\\\u001b[35mFOAF\u001b[0m/LOAF  and bloom filters have a lot of interesting \u001b[35mproperties\u001b[0m for \u001b[35msocial\u001b[0m\\network and whitelist distribution.\\\\I think we can go one level higher though and include \u001b[35mGPG\u001b[0m/\u001b[35mOpenPGP\u001b[0m key\\fingerpring distribution in the FOAF file for simple web-of-trust based \u001b[35mkey\u001b[0m\\distribution.\\\\What \u001b[35mif\u001b[0m \u001b[35mwe\u001b[0m used FOAF and included the PGP key fingerprint(s) for identities?\\\u001b[35mThis\u001b[0m could mean a lot.  You include the PGP key fingerprints within the FOAF\\file of your direct friends and then include a bloom filter of the PGP key\\fingerprints of your entire whitelist (the source FOAF file would of course need\\to be encrypted ).\\\\Your whitelist would be populated from the social network as your client\\discovered new identit ...\\\\\n",
            "\n",
            "FOAFKey: FOAF, PGP, Key Distribution, and Bloom Filters \\\\\u001b[94mIMF\u001b[0m/LOAF  and bloom filters have a lot of interesting \u001b[94mchoices\u001b[0m for \u001b[94minvesting\u001b[0m\\\u001b[94mfinancial\u001b[0m network and whitelist distribution.\\\\I think we can go one level higher though and include \u001b[94mICO\u001b[0m/\u001b[94mUSD\u001b[0m key\\fingerpring distribution in the FOAF file for simple web-of-trust based \u001b[94mportfolio\u001b[0m\\distribution.\\\\What \u001b[94mkeywords\u001b[0m used FOAF and included the PGP \u001b[94mblob\u001b[0m key fingerprint(s) for identities?\\\u001b[94mThey\u001b[0m could mean a lot.  You include the PGP key fingerprints within the FOAF\\file of your direct friends and then include a bloom filter of the PGP key\\fingerprints of your entire whitelist (the source FOAF file would of course need\\to be encrypted ).\\\\Your whitelist would be populated from the social network as your client\\discovered new identit ...\\\\\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 7 / 1 / 0 / 8:   9% 9/100 [1:23:26<14:03:36, 556.23s/it]--------------------------------------------- Result 9 ---------------------------------------------\n",
            "\u001b[35mSci/tech (97%)\u001b[0m --> \u001b[91mWorld (98%)\u001b[0m\n",
            "\n",
            "E-mail scam targets police chief \u001b[35mWiltshire\u001b[0m Police warns about \"\u001b[35mphishing\u001b[0m\" after its fraud squad chief was targeted.\n",
            "\n",
            "E-mail scam targets police chief \u001b[91mIslamabad\u001b[0m Police warns about \"\u001b[91munity\u001b[0m\" after its fraud squad chief was targeted.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 8 / 1 / 0 / 9:  10% 10/100 [1:23:49<12:34:22, 502.92s/it]--------------------------------------------- Result 10 ---------------------------------------------\n",
            "\u001b[35mSci/tech (89%)\u001b[0m --> \u001b[92mSports (98%)\u001b[0m\n",
            "\n",
            "Card fraud unit nets 36,000 cards In its first two years, the \u001b[35mUK's\u001b[0m dedicated card fraud unit, has recovered 36,000 stolen cards and 171 arrests - and estimates it saved 65m.\n",
            "\n",
            "Card fraud unit nets 36,000 cards In its first two years, the \u001b[92mNRL\u001b[0m dedicated card fraud unit, has recovered 36,000 stolen cards and 171 arrests - and estimates it saved 65m.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 9 / 1 / 0 / 10:  11% 11/100 [1:25:36<11:32:41, 466.98s/it]--------------------------------------------- Result 11 ---------------------------------------------\n",
            "\u001b[35mSci/tech (99%)\u001b[0m --> \u001b[94mBusiness (52%)\u001b[0m\n",
            "\n",
            "Group to \u001b[35mPropose\u001b[0m New High-Speed Wireless \u001b[35mFormat\u001b[0m  LOS ANGELES (Reuters) - A group of technology companies  including Texas Instruments Inc. &lt;TXN.N&gt;, STMicroelectronics  &lt;STM.PA&gt; and Broadcom Corp. &lt;BRCM.O&gt;, on Thursday said they  will propose a new wireless networking standard up to 10 times  the speed of the current generation.\n",
            "\n",
            "Group to \u001b[94mBid\u001b[0m New High-Speed Wireless \u001b[94mNew\u001b[0m  LOS ANGELES (Reuters) - A group of technology companies  including Texas Instruments Inc. &lt;TXN.N&gt;, STMicroelectronics  &lt;STM.PA&gt; and Broadcom Corp. &lt;BRCM.O&gt;, \u001b[94minvestors\u001b[0m on Thursday said they  will propose a new wireless networking standard up to 10 times  the speed of the current generation.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 10 / 1 / 0 / 11:  12% 12/100 [1:27:09<10:39:12, 435.82s/it]--------------------------------------------- Result 12 ---------------------------------------------\n",
            "\u001b[35mSci/tech (99%)\u001b[0m --> \u001b[94mBusiness (59%)\u001b[0m\n",
            "\n",
            "\u001b[35mApple\u001b[0m Launches Graphics Software, Video Bundle  LOS ANGELES (Reuters) - Apple Computer Inc.&lt;AAPL.O&gt; on  Tuesday began shipping a new program designed to let users  create real-time motion graphics and unveiled a discount  video-editing software bundle featuring its flagship Final Cut  Pro software.\n",
            "\n",
            "\u001b[94mGoogle\u001b[0m \u001b[94mFinance\u001b[0m Launches Graphics Software, Video Bundle  LOS ANGELES (Reuters) - Apple Computer Inc.&lt;AAPL.O&gt; \u001b[94mInvestors\u001b[0m on  Tuesday began shipping a new program designed to let users  create real-time motion graphics and unveiled a discount  video-editing software bundle featuring its flagship Final Cut  Pro software.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 11 / 1 / 0 / 12:  13% 13/100 [1:28:56<9:55:13, 410.50s/it] --------------------------------------------- Result 13 ---------------------------------------------\n",
            "\u001b[35mSci/tech (100%)\u001b[0m --> \u001b[94mBusiness (83%)\u001b[0m\n",
            "\n",
            "Dutch Retailer Beats Apple to Local \u001b[35mDownload\u001b[0m \u001b[35mMarket\u001b[0m  AMSTERDAM (Reuters) - Free Record Shop, a Dutch music  retail chain, beat Apple Computer Inc. to \u001b[35mmarket\u001b[0m on Tuesday  with the launch of a new download service in Europe's latest  battleground for digital song services.\n",
            "\n",
            "Dutch Retailer Beats Apple to Local \u001b[94mStock\u001b[0m \u001b[94mRankings\u001b[0m  AMSTERDAM (Reuters) - \u001b[94mSirius\u001b[0m Free Record Shop, a Dutch music  retail chain, beat Apple Computer Inc. to \u001b[94mstocks\u001b[0m on Tuesday  with the launch of a new download service in Europe's latest  battleground for digital song services.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 12 / 1 / 0 / 13:  14% 14/100 [1:29:32<9:09:59, 383.72s/it]--------------------------------------------- Result 14 ---------------------------------------------\n",
            "\u001b[35mSci/tech (95%)\u001b[0m --> \u001b[91mWorld (65%)\u001b[0m\n",
            "\n",
            "Super ant colony hits Australia A giant 100km colony of ants  which has been discovered in Melbourne, Australia, could threaten local \u001b[35minsect\u001b[0m \u001b[35mspecies\u001b[0m.\n",
            "\n",
            "Super ant colony hits Australia A giant 100km colony of ants  which has been discovered in Melbourne, Australia, could threaten local \u001b[91mcricket\u001b[0m \u001b[91mfacilities\u001b[0m.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 13 / 1 / 0 / 14:  15% 15/100 [1:29:59<8:29:57, 359.97s/it]--------------------------------------------- Result 15 ---------------------------------------------\n",
            "\u001b[35mSci/tech (97%)\u001b[0m --> \u001b[91mWorld (51%)\u001b[0m\n",
            "\n",
            "Socialites unite \u001b[35mdolphin\u001b[0m groups Dolphin groups, or \"pods\", rely on socialites to keep them from collapsing, \u001b[35mscientists\u001b[0m claim.\n",
            "\n",
            "Socialites unite \u001b[91mlabour\u001b[0m groups Dolphin groups, or \"pods\", rely on socialites to keep them from collapsing, \u001b[91mcharities\u001b[0m claim.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 14 / 1 / 0 / 15:  16% 16/100 [1:30:14<7:53:47, 338.42s/it]--------------------------------------------- Result 16 ---------------------------------------------\n",
            "\u001b[35mSci/tech (86%)\u001b[0m --> \u001b[91mWorld (71%)\u001b[0m\n",
            "\n",
            "Teenage T. rex's monster growth \u001b[35mTyrannosaurus\u001b[0m rex achieved its massive size due to an enormous growth spurt during its adolescent years.\n",
            "\n",
            "Teenage T. rex's monster growth \u001b[91mFreddy\u001b[0m rex achieved its massive size due to an enormous growth spurt during its adolescent years.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 15 / 1 / 0 / 16:  17% 17/100 [1:31:13<7:25:25, 322.00s/it]--------------------------------------------- Result 17 ---------------------------------------------\n",
            "\u001b[35mSci/tech (99%)\u001b[0m --> \u001b[91mWorld (85%)\u001b[0m\n",
            "\n",
            "Scientists Discover Ganymede has a Lumpy Interior Jet \u001b[35mPropulsion\u001b[0m Lab -- Scientists have discovered \u001b[35mirregular\u001b[0m lumps beneath the icy surface of Jupiter's largest moon, Ganymede. These irregular masses may be rock formations, supported by Ganymede's icy shell for billions of years...\n",
            "\n",
            "Scientists Discover Ganymede has a Lumpy Interior Jet \u001b[91mosphere\u001b[0m Lab -- Scientists have discovered \u001b[91munexplained\u001b[0m lumps beneath the icy surface of Jupiter's largest moon, Ganymede. These irregular masses may be rock formations, supported by Ganymede's icy shell for billions of years...\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 16 / 1 / 0 / 17:  18% 18/100 [1:34:10<7:08:59, 313.90s/it]--------------------------------------------- Result 18 ---------------------------------------------\n",
            "\u001b[35mSci/tech (98%)\u001b[0m --> \u001b[91mWorld (61%)\u001b[0m\n",
            "\n",
            "Mars Rovers Relay Images Through Mars Express European Space Agency -- \u001b[35mESAs\u001b[0m \u001b[35mMars\u001b[0m Express has relayed pictures from one of NASA's Mars rovers for the first time, as part of a set of interplanetary networking demonstrations.     The demonstrations pave the way for future Mars missions to draw on joint interplanetary networking capabilities...\n",
            "\n",
            "Mars Rovers Relay Images Through \u001b[91ma\u001b[0m Mars Express \u001b[91mwith\u001b[0m European Space Agency -- \u001b[91mEurope\u001b[0m \u001b[91mSpace\u001b[0m Express has relayed pictures from one of NASA's Mars rovers for the first time, as part of a set of interplanetary networking demonstrations.     The demonstrations \u001b[91marguably\u001b[0m pave the way for future Mars missions to draw on joint interplanetary networking capabilities...\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 17 / 1 / 0 / 18:  19% 19/100 [1:35:46<6:48:18, 302.45s/it]--------------------------------------------- Result 19 ---------------------------------------------\n",
            "\u001b[35mSci/tech (100%)\u001b[0m --> \u001b[94mBusiness (93%)\u001b[0m\n",
            "\n",
            "Rocking the Cradle of Life When did life begin? One evidential clue stems from the fossil records in Western Australia, although whether these layered sediments are biological or chemical has spawned a spirited debate. Oxford researcher, Nicola McLoughlin, describes some of the issues in contention.\n",
            "\n",
            "Rocking the Cradle of \u001b[94mEconomic\u001b[0m Life When did \u001b[94mfinancial\u001b[0m life begin? One evidential clue stems from the fossil records in Western Australia, although whether these layered sediments are biological or chemical has spawned a spirited debate. Oxford \u001b[94meconomics\u001b[0m researcher, Nicola McLoughlin, describes some of the issues in contention.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 18 / 1 / 0 / 19:  20% 20/100 [1:36:21<6:25:24, 289.06s/it]--------------------------------------------- Result 20 ---------------------------------------------\n",
            "\u001b[35mSci/tech (99%)\u001b[0m --> \u001b[94mBusiness (58%)\u001b[0m\n",
            "\n",
            "Storage, \u001b[35mservers\u001b[0m bruise \u001b[35mHP\u001b[0m earnings update Earnings per share rise compared with a year ago, but company misses analysts' expectations by a long shot.\n",
            "\n",
            "Storage, \u001b[94mheadline\u001b[0m bruise \u001b[94mand\u001b[0m earnings update Earnings per share rise compared with a year ago, but company misses analysts' expectations by a long shot.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 19 / 1 / 0 / 20:  21% 21/100 [1:36:35<6:03:22, 275.98s/it]--------------------------------------------- Result 21 ---------------------------------------------\n",
            "\u001b[35mSci/tech (51%)\u001b[0m --> \u001b[94mBusiness (94%)\u001b[0m\n",
            "\n",
            "IBM to hire even more \u001b[35mnew\u001b[0m \u001b[35mworkers\u001b[0m By the end of the year, the computing giant plans to have its biggest headcount since 1991.\n",
            "\n",
            "IBM to hire even more \u001b[94mbankers\u001b[0m By the end of the year, the computing giant plans to have its biggest headcount since 1991.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 20 / 1 / 0 / 21:  22% 22/100 [1:37:33<5:45:52, 266.06s/it]--------------------------------------------- Result 22 ---------------------------------------------\n",
            "\u001b[35mSci/tech (92%)\u001b[0m --> \u001b[91mWorld (58%)\u001b[0m\n",
            "\n",
            "Sun's \u001b[35mLooking\u001b[0m \u001b[35mGlass\u001b[0m \u001b[35mProvides\u001b[0m 3D View Developers get early code for new operating system '\u001b[35mskin'\u001b[0m still being crafted.\n",
            "\n",
            "Sun's \u001b[91mGoogle\u001b[0m \u001b[91mApps\u001b[0m \u001b[91mGets\u001b[0m 3D View Developers get early code for new operating system '\u001b[91mSoftware\u001b[0m still being crafted.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 21 / 1 / 0 / 22:  23% 23/100 [1:38:03<5:28:16, 255.79s/it]--------------------------------------------- Result 23 ---------------------------------------------\n",
            "\u001b[35mSci/tech (98%)\u001b[0m --> \u001b[94mBusiness (97%)\u001b[0m\n",
            "\n",
            "\u001b[35mIBM\u001b[0m \u001b[35mChips\u001b[0m May Someday Heal Themselves New technology applies electrical fuses to help identify and repair faults.\n",
            "\n",
            "\u001b[94mInvest\u001b[0m \u001b[94mInvestors\u001b[0m May Someday Heal Themselves New technology applies electrical fuses to help identify and repair faults.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 22 / 1 / 0 / 23:  23% 23/100 [1:38:03<5:28:16, 255.79s/it]--------------------------------------------- Result 24 ---------------------------------------------\n",
            "\u001b[94mBusiness (64%)\u001b[0m --> \u001b[38:5:240m[SKIPPED]\u001b[0m\n",
            "\n",
            "Some People Not Eligible to Get in on Google IPO Google has billed its IPO as a way for everyday people to get in on the process, denying Wall Street the usual stranglehold it's had on IPOs. Public bidding, a minimum of just five shares, an open process with 28 underwriters - all this pointed to a new level of public participation. But this isn't the case.\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 22 / 1 / 1 / 24:  25% 25/100 [1:38:33<4:55:41, 236.55s/it]--------------------------------------------- Result 25 ---------------------------------------------\n",
            "\u001b[35mSci/tech (99%)\u001b[0m --> \u001b[94mBusiness (98%)\u001b[0m\n",
            "\n",
            "Rivals Try to Turn Tables on Charles Schwab \u001b[35mBy\u001b[0m MICHAEL LIEDTKE     SAN FRANCISCO (AP) -- With its low prices and iconoclastic attitude, discount stock broker Charles Schwab Corp. (SCH) represented an annoying stone in Wall Street's wing-tipped shoes for decades...\n",
            "\n",
            "Rivals Try to Turn Tables on Charles Schwab \u001b[94mGM\u001b[0m MICHAEL LIEDTKE     SAN FRANCISCO (AP) -- With its low prices and iconoclastic attitude, discount stock broker Charles Schwab Corp. (SCH) represented an annoying stone in Wall Street's wing-tipped shoes for decades...\n",
            "\n",
            "\n",
            "[Succeeded / Failed / Skipped / Total] 23 / 1 / 1 / 25:  25% 25/100 [1:38:33<4:55:41, 236.55s/it]"
          ]
        }
      ],
      "source": [
        "!textattack attack --recipe clare --num-examples 100 --model ./outputs/2023-12-12-17-32-40-485239/best_model/  --dataset-from-huggingface ag_news --dataset-split test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text attack did not complete due to lack of resources"
      ],
      "metadata": {
        "id": "Zhc8z3kcVQHS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyrJM3CaseoL"
      },
      "source": [
        "Looks like our model was 94% successful (makes sense - same evaluation set as `textattack eval`!), meaning that TextAttack attacked the model with 94% examples (since the attack won't run if an example is originally mispredicted). The attack success rate was %, meaning that Clare failed to find an adversarial example only % (1 out of ) of the time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xwZqGWz8UVJ"
      },
      "source": [
        "#**Robust models**\n",
        "\n",
        "Adversarial Training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1FpXmrp5V6g"
      },
      "outputs": [],
      "source": [
        "!textattack train --help"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!textattack train --attack clare --model distilbert-base-uncased --dataset ag_news --model-num-labels 4 --model-max-length 64 --per-device-train-batch-size 128 --num-epochs 3"
      ],
      "metadata": {
        "id": "H7wITW7sT7W7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}